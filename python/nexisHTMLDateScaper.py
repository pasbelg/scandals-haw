# -*- coding: utf-8 -*-
"""Copy of news_Zeit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_BuWNhOM79i7d6pxb3BACiU2HRuxPQm

# Manuelle Schritte

- Suchanfrage auf Nexis
- Dokumente als HTML in 200er Batches herunterladen (Limit von Nexis) und Speichern

# Imports und Setup
"""

import os, re
from bs4 import BeautifulSoup
import json
import datetime
import locale

HTMLpath = './html/'
htmlDirs = os.listdir(HTMLpath)
"""# Extrahieren des Textes und der Meta-Daten

## Beispiel der HTML-Struktur eines Dokuments

![html-structure](https://drive.google.com/uc?export=view&id=1fKUW1mqBS4A2wJisigXjbiv0Kg9e1D-E)
![html-structure2](https://drive.google.com/uc?export=view&id=1hRKQSUpAZwlLxN0g1VS5bBMqovEa4l5D)


"""### Eigentliches Extrahieren der Information und ausgabe als JSON"""
locale.setlocale(locale.LC_TIME, '')
def guess_date(string):
        for fmt in ['%d. %B %Y',
                    '%A %d. %B %Y',
                    '%A, %d. %B %Y',
                    '%A %d. %B %Y %I:%M AM %Z',
                    '%A %d. %B %Y %I:%M PM %Z',
                    '%A %d.%B %Y %I:%M AM %Z',
                    '%A %d.%B %Y %I:%M PM %Z',
                    '%A %d. %B %Y %I:%M AM %Z+1',
                    '%A %d. %B %Y %I:%M PM %Z+1',
                    '%d. %B %Y %A %I:%M PM %Z',
                    '%d. %B %Y %A %I:%M AM %Z',
                    '%B %d, %Y %A']:
            try:
                x = datetime.datetime.strptime(string, fmt).date()
                return x.strftime("%d-%m-%Y")
            except ValueError:
                continue
        #raise ValueError(string)
for workingDir in htmlDirs:
    newHTML = os.listdir(HTMLpath + workingDir)
    loopNo = 1
    allData = {}
    dataArray = []
    valueCountEnd=0
    #print(workingDir)
    for f in newHTML:
        filepath=HTMLpath + workingDir + '/' +  f
        if f.find('.HTML') != -1:
            with open(filepath, 'r', encoding="utf8") as html:
                soup = BeautifulSoup(html, "html.parser")
                documents = soup.findAll('docfull')
                for doc in documents:
                    valueCountStart = valueCountEnd                                         
                    dateDiv = doc.findAll('div', {'class': 'c3'})
                    for date in dateDiv:
                        try:
                            day = date.find('p', {'class': 'c1'}).text
                            monthYear = date.find('span', {'class': 'c2'}).text
                        except:
                            print(f)
                            continue
                        combinedDate = monthYear.strip()                   
                        cleanDate = guess_date(combinedDate)
                        valueCountEnd +=1
                        dataArray.append(cleanDate)

                    #allData[loopNo]=dataArray[valueCountStart:valueCountEnd]
                    loopNo += 1  
                try:  
                    os.mkdir('./json')
                except OSError:  
                    print ("Creation of the directory failed")
                try:  
                    os.mkdir('./json/' + workingDir)
                except OSError:  
                    print ("Creation of the directory failed")
                with open('./json/'+workingDir+'/'+ 'data.json', 'w') as outfile:
                    json.dump(dataArray, outfile, indent=1, ensure_ascii=False)
